{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling in Missing Values in Tabular Records\n",
    "\n",
    "You can select Run->Run All Cells from the menu to run all cells in Studio (or Cell->Run All in a SageMaker Notebook Instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Missing data values are common due to omissions during manual entry or optional input. Simple data imputation such as using the median/mode/average may not be satisfactory. When there are many features, we can sometimes train a model to use the existing features to predict the desired feature. \n",
    "\n",
    "This solution provides and end-to-end example that takes a tabular data set with a target column, trains and deploys an endpoint, and calls that endpoint to make predictions.\n",
    "\n",
    "## Architecture\n",
    "As part of the solution, the following services are used:\n",
    "\n",
    "* Amazon S3: Used to store datasets.\n",
    "* Amazon SageMaker Notebook: Used to preprocess and process the data, and to train the deep learning model.\n",
    "* Amazon SageMaker Endpoint: Used to deploy the trained model.\n",
    "\n",
    "![](docs/architecture.png)\n",
    "\n",
    "## Data Set\n",
    "We will use public data from the City of Cincinnati Public Services describing Fleet Inventory. We will train a model to predict missing values of a 'target' column based on the other columns.\n",
    "\n",
    "Please see.\n",
    "https://www.cincinnati-oh.gov/public-services/about-public-services/fleet-services/\n",
    "https://data.cincinnati-oh.gov/Thriving-Neighborhoods/Fleet-Inventory/m8ba-xmjz\n",
    "\n",
    "## Acknowledgements\n",
    "AutoPilot code based on\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/autopilot/sagemaker_autopilot_direct_marketing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these with your train/test CSV data and target columns. \n",
    "# If left empty, the sample data set will be used.\n",
    "data_location = ''    # Ex. s3://your_bucket/your_file.csv\n",
    "target = ''           # Specify target column name\n",
    "\n",
    "if data_location == '':\n",
    "    # Use sample dataset.\n",
    "    dataset_file = 'data/dataset.csv'\n",
    "    target = 'ASSET_TYPE'\n",
    "else:\n",
    "    # Download custom dataset.\n",
    "    !aws s3 cp $data_location data/custom_dataset.csv\n",
    "    print('Downloaded custom dataset')\n",
    "    dataset_file = 'data/custom_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(dataset_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "Some of the entries in the target column are null. We will remove those entries for training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_null_rows(data, target):\n",
    "    idx = data[target].notna()\n",
    "    return data.loc[idx]\n",
    "\n",
    "def split_train_test(data, p=.9):\n",
    "    idx = np.random.choice([True, False], replace = True, size = len(data), p=[.8, .2])\n",
    "    train_df = data.iloc[idx]\n",
    "    test_df = data.iloc[[not i for i in idx]]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_data = remove_null_rows(data, target)\n",
    "train, test = split_train_test(non_null_data)\n",
    "\n",
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "\n",
    "train.to_csv(train_file, index=False, header=True)\n",
    "test.to_csv(test_file, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Processed Data on S3\n",
    "\n",
    "Now that we have our data in files, we store this data to S3 so we can use SageMaker AutoPilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import json\n",
    "\n",
    "with open('stack_outputs.json') as f:\n",
    "    sagemaker_configs = json.load(f)\n",
    "    \n",
    "s3_bucket = sagemaker_configs['S3Bucket']\n",
    "\n",
    "train_data_s3_path = S3Uploader.upload(train_file, 's3://{}/data'.format(s3_bucket))\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "test_data_s3_path = S3Uploader.upload(test_file, 's3://{}/data'.format(s3_bucket))\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure AutoPilot\n",
    "\n",
    "For the purposes of a demo, we will use only 2 candidates. Remove this parameter to run AutoPilot with its defaults (note: for this data set a full run will take ~ 4 several hours.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/data/train'.format(s3_bucket)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': target\n",
    "}]\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': 's3://{}/data/output'.format(s3_bucket)\n",
    "  }\n",
    "automl_job_config ={\n",
    "          'CompletionCriteria': {\n",
    "              'MaxCandidates': 2  # Remove this option for the default run.\n",
    "          }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "role = sagemaker_configs['SageMakerIamRole']\n",
    "\n",
    "solution_prefix = sagemaker_configs['SolutionPrefix']\n",
    "\n",
    "auto_ml_job_name = solution_prefix + strftime('%d-%H-%M-%S', gmtime())\n",
    "print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker',region_name='us-west-2')\n",
    "sm.create_auto_ml_job(AutoMLJobName=auto_ml_job_name,\n",
    "                      InputDataConfig=input_data_config,\n",
    "                      OutputDataConfig=output_data_config,\n",
    "                      AutoMLJobConfig=automl_job_config,\n",
    "                      RoleArn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This will take approximately 20 minutes to run.\n",
    "secondary_status = ''\n",
    "while True:\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "    if job_run_status in ('Failed', 'Completed', 'Stopped'):\n",
    "        print('\\n{}: {}'.format(describe_response['AutoMLJobSecondaryStatus'], job_run_status))\n",
    "        break\n",
    "\n",
    "    if secondary_status == describe_response['AutoMLJobSecondaryStatus']:\n",
    "        print('.', end='')        \n",
    "    else:\n",
    "        secondary_status = describe_response['AutoMLJobSecondaryStatus']\n",
    "        print('\\n{}: {}'.format(secondary_status, job_run_status), end='')\n",
    "    \n",
    "    sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "print(best_candidate)\n",
    "print('\\n')\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker_configs['SageMakerModelName']\n",
    "\n",
    "model = sm.create_model(Containers=best_candidate['InferenceContainers'],\n",
    "                            ModelName=model_name,\n",
    "                            ExecutionRoleArn=role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building endpoint with model {}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = sagemaker_configs['SageMakerEndpointName'] + '-config'\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m5.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = sagemaker_configs['SageMakerEndpointName']\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    )\n",
    "print(create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print('Creating Endpoint... this may take several minutes')\n",
    "while status=='Creating':\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print('.', end='')\n",
    "    time.sleep(15) \n",
    "print(\"\\nStatus: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "test_input = test.drop(columns=[target])[0:10]\n",
    "test_input_csv = test_input.to_csv(index=False, header=False).split('\\n')\n",
    "test_labels = test[target][0:10]\n",
    "\n",
    "\n",
    "\n",
    "for i, (single_test, single_label) in enumerate(zip(test_input_csv, test_labels)):\n",
    "    print('=== Test {} ===\\nInput: {}\\n'.format(i, single_test))    \n",
    "    response = runtime_client.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                        ContentType = 'text/csv',\n",
    "                                        Body = single_test)\n",
    "    result = response['Body'].read().decode('ascii')\n",
    "    print('Predicted label is {}\\nCorrect label is {}\\n'.format(result.rstrip(), single_label.rstrip()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack deletion will clean up all created resources including S3 buckets, Endpoint configurations, Endpoints and Models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}